# -*- coding: utf-8 -*-
"""Lab_3B_graph_neural_networks.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jkNvVbkgkSKJSrOO--UKVEPZC7g3R4yO

# Lab 3 Collaborative Filtering with Graph Neural Networks

The objective of this lab is to design a recommendation system that predicts the ratings that customers would give to a certain product. Say, the rating that a moviegoer would give to a specific movie, or the rating that an online shopper would give to a particular offering. To make these predictions we leverage the ratings that customers have given to this or similar products in the past. This approach to ratings predictions is called collaborative filtering.

We model collaborative filtering as a problem that involves a graph $\mathbf{S}$ and graph signals $\mathbf{x}_u$ supported on the nodes of the graph.
Nodes $p$ of the graph $\mathbf{S}$ represent different products and the weighted edges $S_{pq}$ denote simlarities between products $p$ and $q$. The entries of the graph signal $\mathbf{x}_u$ represent the ratings that user $u$ has given to each product.

The motivation for developing a recommendation system is that customers have rated a subset of prodcuts. Each of us has seen a small number of movies or bought a small number of offerings. Thus, the ratings vector $\mathbf{x}_u$ contains only some entries that correspond to rated products. For the remaining entries we adopt the convention that $x_{ui}=0$. Our goal is to learn a function $\Phi(\mathbf{x}_{u}; \mathcal{H})$ that takes as input the vector $\mathbf{x}_u$ of available ratings and fills in predictions for the ratings that are not available.

We will develop and evaluate a graph neural network (GNN) for making these rating predictions.

# Graph Neural Networks

Graph Neural Networks (GNNs) are information processing architectures made up of a composition of layers, each of which is itself the composition of a linear graph filter with a pointwise nonlinearity.

For a network with a given number of layers $L$ we define the input output relationship through the recursion

\begin{equation}
   \mathbf{X}_\ell
      = \sigma \Big(\, \mathbf{Z}_\ell       \,\Big)
      = \sigma \left(\, \sum_{k=0}^{K_\ell} \mathbf{S}^k \mathbf{X}_{l-1} \mathbf{H}_{\ell k}  \, \right)  ,\quad
\end{equation}

In this recursion the output of Layer $\ell-1$ is $\mathbf{X}_{l-1}$ and it is recast as an input to Layer $\ell$. In this layer, the input $\mathbf{X}_{l-1}$ is processed with a graph filter to produce the intermediate output $\mathbf{Z}_\ell$. The coefficients of this graph filter are the matrices $\mathbf{H}_{\ell k}$. This intermediate output is processed with a pointwise nonlinearity $\sigma$ to produce the output $\mathbf{X}_\ell $ of Layer $\ell$. That the nonlinear operation is pointwise means that it is acting separately on each entry of $\mathbf{Z}_l$.



To complete the recursion we redefine the input $\mathbf{X}$ as the output of Layer 0, $\mathbf{X}_0 = \mathbf{X}$. The output of the neural network is the output of layer $L$, $\mathbf{X}_L = \mathbf{\Phi}(\mathbf{X}; \mathcal{H})$. In this notation $\mathbb{H}$ is the tensor $\mathcal{H} := [\mathbf{H}_{11},\ldots,\mathbf{H}_{LK{\ell}}]$ that groups all of the filters that are used at each of the $L$ layers.

To specify a GNN we need to specify the number of layers $L$ and the characteristics of the filters that are used at each layer. The latter are the number of filter taps $K_\ell$ and the number of features $F_\ell$ at the output of the layer. The number of features $F_0$ must match the number of features at the input and the number of features $F_L$ must match the number of features at the output. Observe that the number of features at the output of Layer $(\ell-1)$ determines the number of features at the input of Layer $\ell$. Then, the filter coefficients at Layer $\ell$ are of dimension $F_{\ell-1} \times F_\ell$.

## 0: Environment setup

Before we beging we need to import the necessary Python Packages. We use Numpy to load data, pytorch for training, and matplotlib to plot and visualize results.
"""

from google.colab import drive
import os

# This will prompt for authorization.
drive.mount('/content/drive')

## Specify the directory where the data is located. Change "ese2000/Lab3B" to your own folder name.
folder_path = '/content/drive/My Drive/ese2000/Lab3B'

# You can then change the directory to this folder to perform operations within it
os.chdir(folder_path)

# To run this notebook on a local computer, go back to the Jupyter home page and click the upload
# button. A dialog box will appear. Locate the file movie_data.p and upload.
#
# To run on Google Colab, upload the movie_data.p file to a folder in your Google Drive and
# uncomment the next four lines of code. The code assumes the movie_data.p file is in a folder
# labeled "ese2000/Lab3B." Change the name to the proper folder if needed.

import numpy as np
import torch; torch.set_default_dtype(torch.float64)
import torch.nn as nn
import torch.optim as optim
import copy
import pickle as pkl
import math
import matplotlib.pyplot as plt
from tqdm import tqdm

# Computations are getting intensive. The amount of samples that we are processing, the size of each
# individual sample and the complexity of the learning parameterization are all much larger than in
# Lab 1. The first line below sets up the notebook to use a GPU if it is available. The print statement
# will enable you to see the device type that you are using once the cell is run

device = 'cuda' if torch.cuda.is_available() else 'cpu'
print(f"Using device: {device}")

"""
## Code from Lab 3A: data loading, loss, and Graph Filters (Run Cells)

"""

# The following command loads the data. For this to work the file "movie_data.p"
# must be uploaded to this notebook. To do that, navigate to the Jupyter Notebook
# interface home page and use the “Upload” option.
with open("movie_data.p", "rb") as f:
    contents = pkl.load(f)
data = contents['data']

# The data file contains a graph of movie similarities. The graph has P nodes.
# This is the graph we will use to run the GNN.
S = data['S'].to(device)
P = S.shape[0]

# The data file contains a training set with entries (x_n, y_n, p_n).
# These entries are stored in the arrays xTrain, yTrain, and pTrain,
# respectively. All of these arrays have nTrain columns, with each
# column corresponding to a different entry in the dataset.
x_train = data['train']['x'].to(device)  # Available ratings for making predictions
y_train = data['train']['y'].to(device)  # Ratings to be predicted
p_train = data['train']['p'].to(device)  # Indices of the movies whose ratings are being predicted
n_train = x_train.shape[0]               # Number of training samples

# Load test data in the same manner
x_test = data['test']['x'].to(device)
y_test = data['test']['y'].to(device)
p_test = data['test']['p'].to(device)
n_test = x_test.shape[0]                 # Number of test samples

print(f"Number of movies (nodes in graph): {P}")
print(f"Number of training samples: {n_train}")
print(f"Number of test samples: {n_test}")

def movie_mse_loss(y_hat, y, idx_movie):
    """
    Evaluates the mean squared error between the predicted ratings and the actual ratings.

    Inputs:
      y_hat: torch.Tensor (batch_size x 1 x num_movies)
        A set of rating estimates, including estimates of all movies.
      y: torch.Tensor (batch_size x 1)
        Actual ratings of specific movies.
      idx_movie: torch.Tensor (batch_size x 1)
        Indices of the movies whose ratings are given by y.

    Outputs:
      mse: torch.Tensor
        Computed mean squared error.
    """
    # Remove unnecessary dimensions
    y_hat = y_hat.squeeze()      # Shape: (batch_size x num_movies)
    y = y.squeeze()              # Shape: (batch_size)
    idx_movie = idx_movie.squeeze()  # Shape: (batch_size)

    # Extract predictions for the specific movies
    prediction = y_hat[torch.arange(y_hat.shape[0]), idx_movie.long()]

    # Compute mean squared error
    mse = torch.mean((prediction - y) ** 2)
    return mse

def filter_function(x, h, S, b):
    '''
    Defines a Graph Filter.

    Inputs:
      x: torch.Tensor (batch_size x in_features x num_nodes)
        Input to the graph filter.
      h: torch.Tensor (K x in_features x out_features)
        Filter coefficients.
      S: torch.Tensor (num_nodes x num_nodes)
        Graph Shift Operator (adjacency or similarity matrix).
      b: torch.Tensor (out_features)
        Bias term.

    Outputs:
      y: torch.Tensor (batch_size x out_features x num_nodes)
        Output of the graph filter.
    '''
    # Get dimensions
    B, G, N = x.shape           # B: batch_size, G: in_features, N: num_nodes
    K, _, F = h.shape           # K: filter order, G: in_features, F: out_features

    # Initialize output tensor
    y = torch.zeros((B, N, F), device=device)

    # Perform the graph filter operation
    for k in range(K):
        # Compute S^k * x
        if k == 0:
            S_k = torch.eye(N, device=device)  # S^0 = Identity matrix
        else:
            S_k = torch.matrix_power(S, k)     # Compute S^k

        # Diffuse signal: x_k = x * S^k
        x_k = torch.matmul(x, S_k)

        # Accumulate the filtered signals
        y += torch.matmul(x_k.permute(0, 2, 1), h[k])

    # Add bias and adjust dimensions
    y = y + b
    y = y.permute(0, 2, 1)  # Shape: (batch_size x out_features x num_nodes)

    return y

class FilterModule(nn.Module):
    """
    Implements a Graph Filter Module as a PyTorch nn.Module.
    """

    def __init__(self, K, f_in, f_out):
        """
        Initializes the Graph Filter Module.

        Parameters:
          K (int): Order of the filter (number of filter taps).
          f_in (int): Input feature dimension.
          f_out (int): Output feature dimension.
        """
        super().__init__()
        self.K = K
        self.f_in = f_in
        self.f_out = f_out

        # Initialize filter coefficients (h) and bias (b)
        self.h = nn.Parameter(torch.randn(self.K, self.f_in, self.f_out))
        self.b = nn.Parameter(torch.zeros(self.f_out))

        # Reset parameters
        self.reset_parameters()

    def reset_parameters(self):
        """
        Resets the filter coefficients and bias to default values.
        """
        stdv = 1. / math.sqrt(self.f_in * self.K)
        self.h.data.uniform_(-stdv, stdv)
        self.b.data.uniform_(-stdv, stdv)

    def forward(self, x, S):
        """
        Forward pass of the Graph Filter Module.

        Parameters:
          x (torch.Tensor): Input tensor of shape (batch_size x in_features x num_nodes).
          S (torch.Tensor): Graph Shift Operator.

        Returns:
          torch.Tensor: Output tensor after applying the graph filter.
        """
        return filter_function(x, self.h, S, self.b)

"""# Task 1
Program a class that implements a GNN with $L$ layers. This class receives as initialization parameters a GNN specification consisting of the number of layers $L$ and vectors $[K_1, \ldots, K_L]$ and $[F_0, F_1, \ldots, F_L]$ containing the number of taps and the number of features of each layer.

Endow the class with a method that takes an input feature $\mathbf{X}$ and produces the corresponding output feature $\mathbf{\Phi}(\mathbf{X}; \mathcal{H})$.
"""

class GNNModule(nn.Module):
    """
    Implements a Graph Neural Network (GNN) with L layers.
    """
    def __init__(self, L, k_list, f_list, sigma):
        """
        Initializes the GNN Module.

        Parameters:
          L (int): Number of layers in the GNN.
          k_list (list of int): List containing the number of filter taps for each layer.
          f_list (list of int): List containing the input dimensions for each layer.
                                Length of f_list should be L + 2.
          sigma (nn.Module): Nonlinear activation function (same for all layers).
        """
        super().__init__()
        # Non-linearity
        self.sigma = sigma

        # This is where we store all the layers in our GNN
        gml = []
        # Iterate through all layers and append them to the list
        for layer in range(L - 1):
            # Create a FilterModule for each layer and append it to the list
            gml.append(FilterModule(k_list[layer], f_list[layer], f_list[layer + 1]).to(device))
        self.gml = gml
        # Add the last linear readout layer
        self.readout = FilterModule(1, f_list[layer + 1], f_list[layer + 2])

    def forward(self, x, S):
        """
        Defines the forward pass of the GNN.

        Parameters:
          x (torch.Tensor): Input tensor of shape (batch_size x in_features x num_nodes).
          S (torch.Tensor): Graph Shift Operator.

        Returns:
          torch.Tensor: Output tensor after passing through the GNN.
        """
        for layer in self.gml:
            # Apply the graph filter
            x = layer(x, S)
            # Apply the non-linearity
            x = self.sigma(x)
        # Apply the linear readout layer
        x = self.readout(x, S)
        return x

"""# Task 2

Train a GNN to predict movie ratings. Plot the evolution of the training loss and evaluate the loss in the test dataset. To obtain a good loss we need to experiment with the number of layers and the number of filter taps per layer.

First, we instantiate a GNN with the following characteristics:


*   Number of layers: 3
*   Filter order at each layer: 5, 5, 1
*   Input dimension at each layer: 1, 64, 32, 1
*   Non-linearity: ReLU(x) = max(0, x)
"""

# Define GNN specifications
L = 3  # Number of layers
K_list = [5, 5, 1]           # Number of filter taps per layer
F_list = [1, 64, 32, 1]      # Number of features per layer (input and output dimensions)
sigma = nn.ReLU()            # Activation function

# Instantiate the GNN model
model = GNNModule(L, K_list, F_list, sigma).to(device)

"""Below we define the training parameters and specify the optimizer. The training procedure has the following charasteristics:

*   Number of epochs (full passes over the dataset): 8
*   Initial Learning Rate: 0.01
*   Optimizer: ADAM
*   Batch Size: 20
"""

# Learning parameters
n_epochs = 8
learning_rate = 0.05
batch_size = 20

# Define the optimizer
optimizer = optim.Adam(model.parameters(), lr=learning_rate)

"""
## Training loop

Below is the main training loop implementation. This is a very straightforward implementation based on a fixed number of epochs.
"""

# Helper variables for data loading during training
batch_index = np.append(np.arange(0, n_train, batch_size), n_train)
n_batches = int(np.ceil(n_train / batch_size))

loss_train = []  # List to store training loss values

for epoch in range(n_epochs):
    # Shuffle the training data indices at the beginning of each epoch
    random_permutation = np.random.permutation(n_train)
    idx_epoch = [int(i) for i in random_permutation]
    epoch_loss = 0  # Initialize epoch loss

    # Iterate over all batches
    for batch in tqdm(range(n_batches)):
        # Determine batch indices for the current batch
        this_batch_indices = idx_epoch[batch_index[batch]: batch_index[batch + 1]]

        # Get the samples in this batch
        x_train_batch = x_train[this_batch_indices, :, :]  # Shape: (batch_size x 1 x num_nodes)
        y_train_batch = y_train[this_batch_indices]        # Shape: (batch_size x 1)
        p_train_batch = p_train[this_batch_indices]        # Shape: (batch_size x 1)

        # Zero the parameter gradients
        model.zero_grad()

        # Forward pass: compute predicted y_hat by passing x_train_batch and S to the model
        y_hat_train_batch = model(x_train_batch, S)        # Output shape: (batch_size x 1 x num_nodes)

        # Compute the loss using the movie_mse_loss function
        loss_value_train = movie_mse_loss(y_hat_train_batch, y_train_batch, p_train_batch)

        # Backward pass: compute gradient of the loss with respect to model parameters
        loss_value_train.backward()

        # Update parameters using the optimizer
        optimizer.step()

        # Accumulate loss for the current epoch
        epoch_loss += loss_value_train.item()

        # Record the loss for visualization purposes
        loss_train.append(loss_value_train.item())

    # Compute average loss for the epoch
    avg_epoch_loss = epoch_loss / n_batches

    # Print training statistics for the current epoch
    print(f"Epoch: {epoch + 1}/{n_epochs} \t Loss: {avg_epoch_loss:.4f}\n")

"""### Training error evolution

We use matplotlib to visualize the evolution of the mean squared error during training.
"""

# Plot the training loss over iterations
plt.figure(figsize=(8, 8))
plt.plot(loss_train, label='Training MSE')
plt.xlabel('Batches')
plt.ylabel('Mean Squared Error')
plt.title('GNN Training Loss Evolution')
plt.legend()
plt.show()

"""
## Evaluation

We evaluate the testing accuracy for the GNN."""

# Evaluate the model on the test data
with torch.no_grad():
    y_hat_test = model(x_test, S)
    test_mse = movie_mse_loss(y_hat_test, y_test, p_test)

print(f"GNN Test Mean Squared Error: {test_mse.item():.5f}")